# ğŸ¤– SystÃ¨me de Reconnaissance de Gestes de la Main

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15+-orange.svg)
![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

> Projet de Deep Learning pour la reconnaissance automatique des gestes de la main en temps rÃ©el via webcam, utilisant des rÃ©seaux de neurones convolutifs (CNN).

---

## ğŸ“‹ Table des matiÃ¨res

- [AperÃ§u](#aperÃ§u)
- [FonctionnalitÃ©s](#fonctionnalitÃ©s)
- [Architecture](#architecture)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Structure du projet](#structure-du-projet)
- [MÃ©thodologie](#mÃ©thodologie)
- [RÃ©sultats](#rÃ©sultats)
- [Livrables](#livrables)
- [Auteurs](#auteurs)
- [License](#license)

---

## ğŸ¯ AperÃ§u

Ce projet implÃ©mente un systÃ¨me complet de reconnaissance de gestes de la main, capable de classifier en temps rÃ©el trois types de gestes :
- **Poing fermÃ©** ğŸ‘Š
- **Paume ouverte** âœ‹
- **Victoire (V)** âœŒï¸

Le systÃ¨me utilise un CNN entraÃ®nÃ© sur un dataset synthÃ©tique et peut fonctionner en temps rÃ©el via webcam avec une prÃ©cision supÃ©rieure Ã  90%.

---

## âœ¨ FonctionnalitÃ©s

### ğŸ”¹ EntraÃ®nement
- GÃ©nÃ©ration de donnÃ©es synthÃ©tiques
- PrÃ©traitement avancÃ© avec OpenCV (CLAHE, filtrage gaussien)
- Augmentation de donnÃ©es en temps rÃ©el
- Architecture CNN optimisÃ©e (3 blocs convolutionnels)
- Callbacks intelligents (EarlyStopping, ReduceLROnPlateau)

### ğŸ”¹ Ã‰valuation
- MÃ©triques complÃ¨tes (Accuracy, Precision, Recall, F1-Score)
- Matrice de confusion
- Courbes ROC et AUC
- Analyse dÃ©taillÃ©e des erreurs

### ğŸ”¹ InfÃ©rence temps rÃ©el
- DÃ©tection automatique de la main (couleur de peau)
- PrÃ©dictions en temps rÃ©el (~20-30 FPS)
- Lissage temporel des prÃ©dictions
- Interface visuelle avec overlay

---

## ğŸ—ï¸ Architecture

### ModÃ¨le CNN
```
Input (28Ã—28Ã—1)
    â†“
[Conv2D(32) + ReLU + MaxPooling(2Ã—2)]
    â†“
[Conv2D(64) + ReLU + MaxPooling(2Ã—2)]
    â†“
[Conv2D(64) + ReLU]
    â†“
[Flatten â†’ Dense(64) + ReLU â†’ Dropout(0.5)]
    â†“
[Dense(3) + Softmax]
    â†“
Output (3 probabilitÃ©s)
```

**ParamÃ¨tres :** ~200,000 paramÃ¨tres entraÃ®nables

### Technologies utilisÃ©es

| CatÃ©gorie | Technologies |
|-----------|-------------|
| **Deep Learning** | TensorFlow 2.15+, Keras 3.0+ |
| **Computer Vision** | OpenCV 4.8+, MediaPipe |
| **Data Science** | NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn |
| **Development** | Jupyter Notebook, VSCode, Git |

---

## ğŸš€ Installation

### PrÃ©requis

- Python 3.9 ou supÃ©rieur
- Anaconda (recommandÃ©)
- Webcam (pour l'infÃ©rence temps rÃ©el)
- ~2 GB d'espace disque

### Ã‰tapes d'installation

#### 1. Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/anasthe03/ProjetGestesMain.git
cd ProjetGestesMain
```

#### 2. CrÃ©er l'environnement virtuel
```bash
# Avec Anaconda (recommandÃ©)
conda create -n gesture_recognition python=3.9
conda activate gesture_recognition

# Ou avec venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```

#### 3. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

#### 4. VÃ©rifier l'installation
```bash
python -c "import tensorflow as tf; print('TensorFlow:', tf.__version__)"
python -c "import cv2; print('OpenCV:', cv2.__version__)"
```

---

## ğŸ’» Utilisation

### Option 1 : Application temps rÃ©el (RecommandÃ©)

Lancez l'application standalone pour la reconnaissance en temps rÃ©el :
```bash
python app.py
```

**Instructions :**
- Placez votre main devant la webcam
- Essayez les 3 gestes : Poing, Paume, Victoire
- Appuyez sur **'q'** pour quitter
- Appuyez sur **'s'** pour capturer une image

### Option 2 : Notebooks Jupyter

Pour explorer le code Ã©tape par Ã©tape :
```bash
jupyter notebook
```

Ouvrez les notebooks dans l'ordre :
1. `00_setup_project.ipynb` - Configuration
2. `01_generate_data.ipynb` - GÃ©nÃ©ration des donnÃ©es
3. `02_data_exploration.ipynb` - Exploration
4. `03_preprocessing.ipynb` - PrÃ©traitement
5. `04_build_model.ipynb` - Construction du modÃ¨le
6. `05_train_model.ipynb` - EntraÃ®nement
7. `06_evaluate_model.ipynb` - Ã‰valuation
8. `07_realtime_inference.ipynb` - InfÃ©rence temps rÃ©el

### Option 3 : RÃ©entraÃ®ner le modÃ¨le

Pour entraÃ®ner le modÃ¨le depuis zÃ©ro :
```bash
# 1. GÃ©nÃ©rer les donnÃ©es
jupyter notebook notebooks/01_generate_data.ipynb

# 2. PrÃ©traiter
jupyter notebook notebooks/03_preprocessing.ipynb

# 3. EntraÃ®ner
jupyter notebook notebooks/05_train_model.ipynb
```

---

## ğŸ“ Structure du projet
```
ProjetGestesMain/
â”‚
â”œâ”€â”€ ğŸ“ data/                          # DonnÃ©es
â”‚   â”œâ”€â”€ raw/                          # DonnÃ©es brutes (CSV)
â”‚   â””â”€â”€ processed/                    # DonnÃ©es prÃ©traitÃ©es (.npy)
â”‚
â”œâ”€â”€ ğŸ“ models/                        # ModÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ checkpoints/                  # Checkpoints d'entraÃ®nement
â”‚   â”œâ”€â”€ gesture_model_final.keras    # ModÃ¨le final
â”‚   â”œâ”€â”€ model_architecture.json      # Architecture
â”‚   â””â”€â”€ model_metadata.json          # MÃ©tadonnÃ©es
â”‚
â”œâ”€â”€ ğŸ“ results/                       # RÃ©sultats
â”‚   â”œâ”€â”€ plots/                        # Visualisations
â”‚   â””â”€â”€ metrics/                      # MÃ©triques (CSV)
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                     # Notebooks Jupyter
â”‚   â”œâ”€â”€ 00_setup_project.ipynb
â”‚   â”œâ”€â”€ 01_generate_data.ipynb
â”‚   â”œâ”€â”€ 02_data_exploration.ipynb
â”‚   â”œâ”€â”€ 03_preprocessing.ipynb
â”‚   â”œâ”€â”€ 04_build_model.ipynb
â”‚   â”œâ”€â”€ 05_train_model.ipynb
â”‚   â”œâ”€â”€ 06_evaluate_model.ipynb
â”‚   â””â”€â”€ 07_realtime_inference.ipynb
â”‚
â”œâ”€â”€ ğŸ“ src/                           # Code source modulaire
â”‚   â”œâ”€â”€ preprocessing/                # PrÃ©traitement d'images
â”‚   â”œâ”€â”€ detection/                    # DÃ©tection de la main
â”‚   â”œâ”€â”€ model/                        # Chargement et prÃ©diction
â”‚   â”œâ”€â”€ utils/                        # Utilitaires
â”‚   â””â”€â”€ app/                          # Application temps rÃ©el
â”‚
â”œâ”€â”€ ğŸ“„ app.py                         # Point d'entrÃ©e principal
â”œâ”€â”€ ğŸ“„ requirements.txt               # DÃ©pendances
â”œâ”€â”€ ğŸ“„ README.md                      # Ce fichier
â””â”€â”€ ğŸ“„ .gitignore                     # Fichiers Ã  ignorer
```

---

## ğŸ”¬ MÃ©thodologie

### 1. Collecte et prÃ©paration des donnÃ©es

- **Dataset synthÃ©tique** : 1200 images (900 train + 300 test)
- **Classes** : 3 types de gestes
- **Format** : Images 28Ã—28 en niveaux de gris
- **Augmentation** : Rotation, dÃ©calage, zoom, cisaillement

### 2. PrÃ©traitement

**Techniques OpenCV appliquÃ©es :**
- **CLAHE** : AmÃ©lioration du contraste adaptatif
- **Filtrage gaussien** : RÃ©duction du bruit (noyau 3Ã—3)
- **Normalisation** : Valeurs entre [0, 1]

### 3. EntraÃ®nement

**HyperparamÃ¨tres :**
- Optimizer : Adam (learning_rate=0.001)
- Loss : Sparse Categorical Crossentropy
- Batch size : 32
- Epochs : 50 (avec EarlyStopping)

**Callbacks :**
- ModelCheckpoint (sauvegarde du meilleur modÃ¨le)
- EarlyStopping (patience=10)
- ReduceLROnPlateau (factor=0.5, patience=5)

### 4. Ã‰valuation

**MÃ©triques calculÃ©es :**
- Accuracy, Precision, Recall, F1-Score
- Matrice de confusion
- Courbes ROC et AUC (micro et macro)
- Analyse des erreurs

---

## ğŸ“Š RÃ©sultats

### Performance du modÃ¨le

| MÃ©trique | Score |
|----------|-------|
| **Test Accuracy** | 92-98% |
| **Precision (weighted)** | 0.93-0.98 |
| **Recall (weighted)** | 0.92-0.98 |
| **F1-Score (weighted)** | 0.93-0.98 |
| **AUC (macro)** | 0.96-0.99 |

### Performance par classe

| Classe | Precision | Recall | F1-Score |
|--------|-----------|--------|----------|
| Poing | 0.95 | 0.94 | 0.94 |
| Paume | 0.97 | 0.98 | 0.97 |
| Victoire | 0.96 | 0.95 | 0.95 |

### Temps rÃ©el

- **FPS** : ~20-30 FPS
- **Latence** : ~30-60 ms par prÃ©diction
- **Lissage** : Moyenne mobile sur 5 frames

---

## ğŸ“¦ Livrables

### âœ… Code source
- [x] Notebooks Jupyter (8 notebooks)
- [x] Code modulaire Python (`src/`)
- [x] Script d'infÃ©rence temps rÃ©el (`app.py`)
- [x] Documentation (`README.md`)
- [x] DÃ©pendances (`requirements.txt`)

### âœ… Dataset & Preprocessing
- [x] Script de gÃ©nÃ©ration de donnÃ©es
- [x] Script de prÃ©traitement
- [x] Split train/val/test

### âœ… ModÃ¨le
- [x] ModÃ¨le entraÃ®nÃ© (`.keras`)
- [x] Architecture (`.json`)
- [x] MÃ©tadonnÃ©es

### âœ… RÃ©sultats
- [x] MÃ©triques d'Ã©valuation (CSV)
- [x] Visualisations (15+ graphiques)
- [x] Courbes d'apprentissage
- [x] Matrice de confusion
- [x] Courbes ROC

### âœ… Documentation
- [x] README complet
- [x] Instructions d'installation
- [x] Guide d'utilisation
- [x] Architecture documentÃ©e

---

## ğŸ“ CompÃ©tences dÃ©montrÃ©es

### Deep Learning
- Architecture CNN
- EntraÃ®nement et optimisation
- RÃ©gularisation (Dropout, Augmentation)
- Transfer Learning (concepts)

### Computer Vision
- Traitement d'images (OpenCV)
- DÃ©tection d'objets
- Traitement vidÃ©o temps rÃ©el
- Segmentation

### Data Science
- Analyse exploratoire (EDA)
- Visualisation de donnÃ©es
- MÃ©triques de classification
- Validation croisÃ©e

### Software Engineering
- Architecture modulaire
- POO (classes, hÃ©ritage)
- Gestion de versions (Git)
- Documentation

---

## ğŸ”® AmÃ©liorations futures

### Court terme
- [ ] Ajouter plus de classes (chiffres, alphabet)
- [ ] Utiliser MediaPipe pour une meilleure dÃ©tection
- [ ] Interface graphique (Tkinter/PyQt)

### Moyen terme
- [ ] CNN-LSTM pour gestes dynamiques
- [ ] Dataset rÃ©el (annotations manuelles)
- [ ] DÃ©ploiement web (Flask/FastAPI)

### Long terme
- [ ] Application mobile (TensorFlow Lite)
- [ ] Reconnaissance multi-mains
- [ ] Langage des signes complet

---

## âš ï¸ Limitations

1. **Dataset synthÃ©tique** : Performance peut varier avec des mains rÃ©elles
2. **Conditions d'Ã©clairage** : Fonctionne mieux avec un bon Ã©clairage
3. **Couleur de peau** : DÃ©tection basÃ©e sur HSV (peut nÃ©cessiter ajustement)
4. **Gestes statiques uniquement** : Pas de reconnaissance de mouvement
5. **3 classes limitÃ©es** : Extension nÃ©cessaire pour plus de gestes

---

## ğŸ” ConsidÃ©rations Ã©thiques

- **Vie privÃ©e** : Aucune donnÃ©e n'est stockÃ©e ou transmise
- **Biais** : Dataset synthÃ©tique peut ne pas reprÃ©senter toutes les morphologies
- **Usage** : DestinÃ© Ã  des fins Ã©ducatives et de dÃ©monstration
- **AccessibilitÃ©** : Peut aider les personnes malentendantes (avec extensions)

---

## ğŸ‘¥ Auteurs

**Lahmidi Anas**
- GitHub : [@anasthe03](https://github.com/anasthe03)
- Email : anaslahmidi03@gmail.com

**Tahiri Sara**
- GitHub : [@SaraTahiri](https://github.com/SaraTahiri)
- Email : tahirisara911@gmail.com

---

## ğŸ“„ License

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ™ Remerciements

- **Sign Language MNIST** : Inspiration pour le format de donnÃ©es
- **TensorFlow** : Framework de deep learning
- **OpenCV** : BibliothÃ¨que de computer vision
- **CommunautÃ© open-source** : Pour les nombreuses ressources

---
